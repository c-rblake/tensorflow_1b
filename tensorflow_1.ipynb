{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is from Kaggle.com\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age     sex     bmi  children smoker     region\n",
      "0   19  female  27.900         0    yes  southwest\n",
      "1   18    male  33.770         1     no  southeast\n",
      "2   28    male  33.000         3     no  southeast\n",
      "3   33    male  22.705         0     no  northwest\n",
      "4   32    male  28.880         0     no  northwest\n",
      "Number of features: 6\n",
      "Number of samples:  1338\n",
      "           age      bmi  children\n",
      "count  1338.00  1338.00   1338.00\n",
      "mean     39.21    30.66      1.09\n",
      "std      14.05     6.10      1.21\n",
      "min      18.00    15.96      0.00\n",
      "25%      27.00    26.30      0.00\n",
      "50%      39.00    30.40      1.00\n",
      "75%      51.00    34.69      2.00\n",
      "max      64.00    53.13      5.00\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1338 entries, 0 to 1337\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   age       1338 non-null   int64  \n",
      " 1   sex       1338 non-null   object \n",
      " 2   bmi       1338 non-null   float64\n",
      " 3   children  1338 non-null   int64  \n",
      " 4   smoker    1338 non-null   object \n",
      " 5   region    1338 non-null   object \n",
      "dtypes: float64(1), int64(2), object(3)\n",
      "memory usage: 62.8+ KB\n",
      "None\n",
      "(1338,)\n",
      "count     1338.00\n",
      "mean     13270.42\n",
      "std      12110.01\n",
      "min       1121.87\n",
      "25%       4740.29\n",
      "50%       9382.03\n",
      "75%      16639.91\n",
      "max      63770.43\n",
      "Name: charges, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing and Exploration\n",
    "#load the dataset\n",
    "dataset = pd.read_csv('insurance.csv') \n",
    "#first 7 columns as features\n",
    "features = dataset.iloc[:,0:6] \n",
    "#choose the final column for prediction\n",
    "labels = dataset.iloc[:,-1] \n",
    "print(features.head())\n",
    "#print the number of features in the dataset\n",
    "print(\"Number of features:\", features.shape[1]) \n",
    "#print the number of samples in the dataset\n",
    "print(\"Number of samples: \", features.shape[0]) \n",
    "#summary statistics for numeric features\n",
    "print(features.describe().round(2)) \n",
    "print(features.info())\n",
    "print(labels.shape)\n",
    "print(labels.describe().round(2))\n",
    "#print(dataset.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "#from sklearn.preprocessing import LabelEncoder OHE accepts strings since v 0.2\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "dataset = pd.read_csv('insurance.csv')\n",
    "#choose first 7 columns as features\n",
    "features = dataset.iloc[:,0:6]\n",
    "features2 = dataset.iloc[:,0:6]\n",
    "#choose the final column for prediction\n",
    "labels = dataset.iloc[:,-1]\n",
    "#Many models other than decision trees only work with numeric features.\n",
    "#One-hot encoding creates a binary column for each category.\n",
    "#Two common ways of doing this is pandas.get_dummies method on categorical columns or LabelEncoder + OnehotEncoder\n",
    "features = pd.get_dummies(features) \n",
    "# Next cell covers a Pipeline for ColumTransformer over get_dummies. ,\n",
    "# Get dummies is more visual to work with as it also returns a dataframe with column names.\n",
    "\n",
    "#le = LabelEncoder()\n",
    "\n",
    "#split the data into training and test data,\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42) # 42 because Galaxies are fun,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By having features with differing scales, the optimizer might update some weights faster than the others.\n",
    "# To remedy this we can use for example Normalization or Standardization\n",
    "# Case 1: Normalize the numeric columns Normalization is scales the numerical features to a fixed range - usually between 0 and 1,\n",
    "ct = ColumnTransformer([('normalize', Normalizer(), ['age', 'bmi', 'children'])], remainder='passthrough')\n",
    "#fit the normalizer to the training data and convert from numpy arrays to pandas frame\n",
    "features_train_norm = ct.fit_transform(features_train) \n",
    "#applied the trained normalizer on the test data and convert from n umpy arrays to pandas frame,\n",
    "features_test_norm = ct.transform(features_test) \n",
    "# Note: By applying the transformer after, we get punished for not having large enough dataset.\n",
    "# If we do not have enough data to really account for the broad specturum of variability. \n",
    "# The split will have many new types of outliers in our test set and really punished a model in evaluation.\n",
    "\n",
    "#ColumnTransformer returns numpy arrays. Convert the features to dataframes\n",
    "features_train_norm = pd.DataFrame(features_train_norm, columns = features_train.columns)\n",
    "features_test_norm = pd.DataFrame(features_test_norm, columns = features_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          age     bmi  children  sex_female  sex_male  smoker_no  smoker_yes  \\\n",
      "count  896.00  896.00    896.00      896.00    896.00     896.00      896.00   \n",
      "mean    -0.00    0.00     -0.00        0.49      0.51       0.79        0.21   \n",
      "std      1.00    1.00      1.00        0.50      0.50       0.41        0.41   \n",
      "min     -1.49   -2.44     -0.91        0.00      0.00       0.00        0.00   \n",
      "25%     -0.86   -0.71     -0.91        0.00      0.00       1.00        0.00   \n",
      "50%     -0.02   -0.05     -0.08        0.00      1.00       1.00        0.00   \n",
      "75%      0.90    0.66      0.75        1.00      1.00       1.00        0.00   \n",
      "max      1.74    3.78      3.24        1.00      1.00       1.00        1.00   \n",
      "\n",
      "       region_northeast  region_northwest  region_southeast  region_southwest  \n",
      "count            896.00            896.00            896.00            896.00  \n",
      "mean               0.26              0.25              0.26              0.24  \n",
      "std                0.44              0.43              0.44              0.42  \n",
      "min                0.00              0.00              0.00              0.00  \n",
      "25%                0.00              0.00              0.00              0.00  \n",
      "50%                0.00              0.00              0.00              0.00  \n",
      "75%                1.00              1.00              1.00              0.00  \n",
      "max                1.00              1.00              1.00              1.00  \n",
      "          age     bmi  children  sex_female  sex_male  smoker_no  smoker_yes  \\\n",
      "count  442.00  442.00    442.00      442.00    442.00     442.00      442.00   \n",
      "mean    -0.01    0.06     -0.01        0.51      0.49       0.81        0.19   \n",
      "std      0.97    1.06      1.00        0.50      0.50       0.40        0.40   \n",
      "min     -1.49   -2.30     -0.91        0.00      0.00       0.00        0.00   \n",
      "25%     -0.93   -0.71     -0.91        0.00      0.00       1.00        0.00   \n",
      "50%     -0.02    0.01     -0.08        1.00      0.00       1.00        0.00   \n",
      "75%      0.81    0.77      0.75        1.00      1.00       1.00        0.00   \n",
      "max      1.74    3.68      3.24        1.00      1.00       1.00        1.00   \n",
      "\n",
      "       region_northeast  region_northwest  region_southeast  region_southwest  \n",
      "count            442.00            442.00            442.00            442.00  \n",
      "mean               0.21              0.22              0.31              0.26  \n",
      "std                0.41              0.42              0.46              0.44  \n",
      "min                0.00              0.00              0.00              0.00  \n",
      "25%                0.00              0.00              0.00              0.00  \n",
      "50%                0.00              0.00              0.00              0.00  \n",
      "75%                0.00              0.00              1.00              1.00  \n",
      "max                1.00              1.00              1.00              1.00  \n"
     ]
    }
   ],
   "source": [
    "# Case 2: StandardScaler() standardization that rescales features to zero mean and unit variance. \n",
    "my_ct = ColumnTransformer([('scale', StandardScaler(), ['age', 'bmi', 'children'])], remainder='passthrough')\n",
    "# The column transformer object has the columns to work on in it.\n",
    "\n",
    "features_train_scale = my_ct.fit_transform(features_train)\n",
    "features_test_scale = my_ct.transform(features_test)\n",
    "\n",
    "# Transformes the Scaled objects back to DataFrames from Numpy arrays.\n",
    "features_train_scale = pd.DataFrame(features_train_scale, columns=features_train.columns)\n",
    "features_test_scale = pd.DataFrame(features_test_scale, columns=features_test.columns)\n",
    "\n",
    "print(features_train_scale.describe().round(2))\n",
    "print(features_test_scale.describe().round(2)) # Summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('num', SimpleImputer(strategy='constant'),\n",
       "                                 ['age', 'bmi', 'children']),\n",
       "                                ('cat',\n",
       "                                 Pipeline(steps=[('imputer',\n",
       "                                                  SimpleImputer(strategy='most_frequent')),\n",
       "                                                 ('onehot',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                 ['sex', 'smoker', 'region'])])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipeline Version of the above. The below object would be even better Object oriented programming wise\n",
    "ColumnTransformer(transformers=[('num', SimpleImputer(strategy='constant'), ['age', 'bmi', 'children']),\n",
    "('cat', Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n",
    "('onehot',OneHotEncoder(handle_unknown='ignore'))]),['sex', 'smoker', 'region'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColumnTransformer(transformers=[('num', SimpleImputer(strategy='constant'),\n",
      "                                 ['age', 'bmi', 'children']),\n",
      "                                ('cat',\n",
      "                                 Pipeline(steps=[('imputer',\n",
      "                                                  SimpleImputer(strategy='most_frequent')),\n",
      "                                                 ('onehot',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'))]),\n",
      "                                 ['sex', 'smoker', 'region'])])\n"
     ]
    }
   ],
   "source": [
    "#load the dataset\n",
    "dataset2 = pd.read_csv('insurance.csv') \n",
    "#choose first 7 columns as features\n",
    "features2 = dataset.iloc[:,0:6]\n",
    "#choose the final column for prediction\n",
    "target = dataset.iloc[:,-1] # popping is not a good idea here since we are doing parallell \n",
    "# Transformer Pipeline\n",
    "X_train, X_val, y_train, y_val = train_test_split(features2, target, train_size=0.8, test_size=0.2, random_state=0)\n",
    "\n",
    "numerical_cols = [col for col in features2.columns if features2[col].dtype in ['int64', 'float64']]\n",
    "# or pandas .select_dtypes(['int64', 'float64'])\n",
    "#categorical_cols = [col for col in inputs.columns if inputs[col].dtype==object]\n",
    "categorical_cols = [col for col in features2.columns if features2[col].dtype==object and features2[col].nunique() <=10]\n",
    "# Categorical colum does a check for Cardinality. If too many columns are created the dataset becomes very large.\n",
    "\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train[my_cols].copy()\n",
    "X_valid = X_val[my_cols].copy()\n",
    "\n",
    "# PIPELINE OBJECTs are useful for reproducibility and Object oriented code.\n",
    "# TAKES PREPROCESSOR/TRANSFORMER STEPs AND ONE ESTIMATOR STEP. Here it just takes a \n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "('num', numerical_transformer, numerical_cols),\n",
    "('cat', categorical_transformer, categorical_cols)])\n",
    "\n",
    "train_piped = preprocessor.fit(X_train)\n",
    "test_piped = preprocessor.fit(X_val)\n",
    "print(train_piped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer,Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "def linear_model(features): \n",
    "  model = Sequential(name = \"linear model\")\n",
    "  num_features = features.shape[1]\n",
    "  input = InputLayer(input_shape=(num_features,)) # local scope Built-in overwrite \n",
    "  model.add(input) #add the input layer\n",
    "  model.add(Dense(1)) # The One output makes it a Regression model. A continous prediction per observation.\n",
    "  return model\n",
    "\n",
    "def linear_model2(features):\n",
    "  model = Sequential(name= 'linear model 2')\n",
    "  num_features = features.shape[1] # rows, columns\n",
    "  input = InputLayer(input_shape=(num_features,))\n",
    "  model.add(input)\n",
    "  model.add(Dense(128, activation='relu')) # Binary number bases are easier to compute like 2^7 = 128\n",
    "  # recitified linear unit allows for non-linear. It takes the input array and returns max(input, 0) array.\n",
    "  model.add(Dense(1))\n",
    "  opt = Adam(learning_rate=0.01) # General purpose learning optimizer \n",
    "  model.compile(loss='mse',  metrics=['mae'], optimizer=opt) \n",
    "  # Linear Regression Optimizer mimizing Mean Square Error. \n",
    "  # MSE punishes mistakes on outliers more than \\normal\\ data so far in the iterations\n",
    "  # displaying Mean Absolute Error during Training. Easier To understand for Validation\n",
    "  # To model on evaluation will return both MSE and MAE. How far we are on Each Metric\n",
    "  # MSE, MAE = model.evaluate(params) Tuple\n",
    "  return model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Variable 'dense_10/kernel:0' shape=(11, 8) dtype=float32>, <tf.Variable 'dense_10/bias:0' shape=(8,) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "# 896 split samples, 11 features as in our dataset\n",
    "input = tf.ones((896, 11)) \n",
    "# a fully-connected layer with 8 neurons\n",
    "layer = Dense(8)\n",
    "# calculate the outputs\n",
    "output = layer(input)\n",
    "# print the weights\n",
    "print(layer.weights) \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"linear model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 1)                 12        \n",
      "=================================================================\n",
      "Total params: 12\n",
      "Trainable params: 12\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = linear_model(features_train_scale) # Using the Potentially Smaller DataSet adjusted for say Cardinality.,\n",
    "print(model.summary())\n",
    "# 11 inputs, to 9 connected + 3 biases => 12 params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stops Training if No more improvments are made. \n",
    "# This is usually done on a validation set\n",
    "# here I didn't use such a split so it rather acts as a stop for large number of epoch\n",
    "# The utility is lower this way but the functionality is helpful with computational time.\n",
    "# See my tensorflow_3_computational_reducibility for how I interpret and use this functionallity.\n",
    "callbacks = [EarlyStopping(monitor='mean_absolute_error',min_delta=1e-2,patience=2, verbose=1)]\n",
    "\n",
    "# Normally stop training when `val_loss` is no longer improving\n",
    "# no longer improving being defined as \\no better than 1e-2 less\n",
    "# no longer improving being further defined as for at least 2 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00031: early stopping\n",
      "2798.1626\n"
     ]
    }
   ],
   "source": [
    "model2 = linear_model2(features_train_scale)\n",
    "model2.fit(features_train_scale, labels_train, callbacks= callbacks, epochs=40, batch_size=1, verbose=0)\n",
    "# The callback is also using MSE here\n",
    "# Batch of 1 is maximum updates. With more data it should be increased. Vebrose = True for output #9600-> 2800\n",
    "# Callbacks come in list for here defined on assignment\n",
    "\n",
    "val_mse, val_mae = model2.evaluate(features_test_scale, labels_test, verbose=0) # Like predict + score in scikit learn,\n",
    "print(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6316.572]]\n",
      "age                38\n",
      "sex              male\n",
      "bmi             37.05\n",
      "children            1\n",
      "smoker             no\n",
      "region      northeast\n",
      "charges       6079.67\n",
      "Name: 44, dtype: object\n"
     ]
    }
   ],
   "source": [
    "prediction = model2.predict(np.array([features_test_scale.iloc[44],])) # .values.reshape goes to array anyway,\n",
    "print(prediction)\n",
    "print(dataset.iloc[44])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion,\n",
    "Decent performance on this task. We had a Skewed set of data with ,\n",
    "a mean of 13270.42,\n",
    "std  12110.01,\n",
    "One of my metrics is the MAE / std ratio. And here it is about 24% which is way better than chancing it.,\n",
    "Another alternative would be to use a simple linear regression. And use that as a baseline for the model. ,\n",
    "The mean is used with in regression line for determining R2, the impact of the relationships. ,\n",
    "If we have random data the mean and the regression line are the same but on many datasets a regression baseline style could be helpful for determining the usefulness of a Neural Network.,\n",
    "Just like the mean another baseline to evaluate is the median."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
